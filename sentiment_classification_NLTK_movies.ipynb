{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt', 'neg/cv003_12683.txt', 'neg/cv004_12641.txt', 'neg/cv005_29357.txt', 'neg/cv006_17022.txt', 'neg/cv007_4992.txt', 'neg/cv008_29326.txt', 'neg/cv009_29417.txt']\n"
     ]
    }
   ],
   "source": [
    "print(movie_reviews.fileids()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "print (len(movie_reviews.fileids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4043\n"
     ]
    }
   ],
   "source": [
    "print(len(movie_reviews.raw('neg/cv000_29416.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot : two teen couples go to a church party , drink and then drive . \n",
      "they get into an accident . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(movie_reviews.raw('neg/cv000_29416.txt')[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    " \n",
    "stop_words = stopwords.words('english') + list(punctuation)\n",
    " \n",
    "def tokenize(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [w.lower() for w in words]\n",
    "    return [w for w in words if w not in stop_words and not w.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today', 'friday', 'weather', 'sunny']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"today is Friday. The weather is sunny.\") # will remove the stop words and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the vocabulary in one pass\n",
    "vocabulary = set()\n",
    "for file_id in movie_reviews.fileids():\n",
    "    words = tokenize(movie_reviews.raw(file_id))\n",
    "    vocabulary.update(words)\n",
    "    \n",
    "vocabulary = list(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46215\n"
     ]
    }
   ],
   "source": [
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['creepy', 'unjustly', 'belly-button', '3-year', 'amplified', 'fringe', 'elfont', 'impressive-as-ever', 'argentinian', 'firebird']\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate the tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "word_index = {w: idx for idx, w in enumerate(vocabulary)}\n",
    "print(word_index['creepy'])\n",
    "print(word_index['unjustly'])\n",
    "print(word_index['belly-button'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "word_idf = defaultdict(lambda: 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for file_id in movie_reviews.fileids():\n",
    "    words = set(tokenize(movie_reviews.raw(file_id)))\n",
    "    for word in words:\n",
    "        word_idf[word] += 1\n",
    "word_idf[\"creepy\"] #creepy has appears 147 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46215 2000\n",
      "0.2600669054188076\n",
      "3.2441936328524905\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "VOCABULARY_SIZE = len(vocabulary)\n",
    "DOCUMENTS_COUNT = len(movie_reviews.fileids())\n",
    " \n",
    "print(VOCABULARY_SIZE, DOCUMENTS_COUNT)    # 46215 2000\n",
    "\n",
    "for word in vocabulary:\n",
    "    word_idf[word] = math.log(DOCUMENTS_COUNT / float(1 + word_idf[word]))\n",
    " \n",
    "print (word_idf['movie'])    # 8.095775128229402\n",
    "print (word_idf['creepy'])   # 6.318944087481682\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_tf(word, document):\n",
    "    if isinstance(document, str):\n",
    "        document = tokenize(document) \n",
    "    return float(document.count(word)) / len(document)\n",
    " \n",
    "def tf_idf(word, document):\n",
    "    # If not tokenized\n",
    "    if isinstance(document, str):\n",
    "        document = tokenize(document) \n",
    "    if word not in word_index:\n",
    "        return .0 \n",
    "    return word_tf(word, document) * word_idf[word]\n",
    "\n",
    "print(tf_idf(\"accident\", movie_reviews.raw('neg/cv000_29416.txt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get tf-idf by sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0495250118674\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "tfidf = TfidfVectorizer(stop_words=stop_words, tokenizer=tokenize, vocabulary=vocabulary)\n",
    " \n",
    "# Fit the TfIdf model\n",
    "tfidf.fit([movie_reviews.raw(file_id) for file_id in movie_reviews.fileids()])\n",
    " \n",
    "# Transform a document into TfIdf coordinates\n",
    "X = tfidf.transform([movie_reviews.raw('neg/cv000_29416.txt')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check out some frequencies\n",
    "print (X[0, tfidf.vocabulary_['accident']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc, label = [], []\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        doc.append(movie_reviews.raw(fileid))\n",
    "        label.append(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot : two neg\n",
      "the happy  neg\n",
      "it is movi neg\n",
      " \" quest f neg\n",
      "synopsis : neg\n",
      "capsule :  neg\n",
      "so ask you neg\n",
      "that's exa neg\n",
      "call it a  neg\n",
      "plot : a y neg\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(doc[i][:10], label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data to training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(doc, label, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 35393)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'good excellent famous' => pos\n",
      "'very bad poor creepy ' => neg\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['good excellent famous', 'very bad poor creepy ']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test))\n",
    "print(type(X_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "(['no', 'film', 'in', 'recent', 'has', 'left', 'me', 'with', 'such', 'conflicted', 'feelings', 'as', 'neil', 'jordan', \"'\", 's', 'harrowing', ',', 'humorous', ',', 'horrifying', 'adaptation', 'of', 'patrick', 'mccabe', \"'\", 's', 'novel', 'about', 'young', 'lad', 'francie', 'brady', \"'\", 's', '(', 'eamonn', 'owens', ')', 'descent', 'into', 'madness', 'in', '1960s', 'ireland', '.', 'on', 'one', 'hand', ',', 'it', 'was', 'difficult', 'for', 'me', 'to', 'become', 'invested', 'in', 'francie', \"'\", 's', 'story', 'because', 'he', 'is', 'such', 'an', 'unsavory', 'character', ',', 'unjustifyably', 'venting', 'his', 'rage', 'at', 'his', 'nosy', 'but', 'otherwise', 'harmless', 'neighbor', 'mrs', '.', 'nugent', '(', 'fiona', 'shaw', ')', '.', 'on', 'another', 'hand', ',', 'i', 'found', 'it', 'difficult', 'to', 'laugh', 'at', 'some', 'of', 'francie', \"'\", 's', 'darkly', 'comic', 'shenanigans', 'because', 'he', 'obviously', 'is', 'such', 'a', 'sick', ',', 'needy', 'child', ',', 'having', 'been', 'raised', 'by', 'a', 'drunken', 'father', '(', 'stephen', 'rea', ')', 'and', 'a', 'suicidal', 'mother', '(', 'aisling', 'o', \"'\", 'sullivan', ')', '.', 'on', 'yet', 'another', 'hand', ',', 'i', 'also', 'found', 'it', 'difficult', 'to', 'completely', 'sympathize', 'with', 'francie', 'during', 'his', 'more', 'emotional', 'scenes', 'because', 'some', 'of', 'his', ',', 'for', 'lack', 'of', 'a', 'better', 'word', ',', '\"', 'bad', '\"', 'deeds', 'are', 'so', 'incredibly', 'shocking', 'in', 'their', 'brutality', 'and', 'the', 'malicious', 'glee', 'in', 'which', 'he', 'performs', 'them', '.', 'however', ',', 'the', 'butcher', 'boy', \"'\", 's', 'power', 'is', 'undeniable', ',', 'and', 'the', 'film', 'as', 'a', 'whole', 'is', 'unforgettable', '--', 'perhaps', 'because', 'it', 'is', 'so', 'disturbing', '.', 'what', 'makes', 'it', 'so', 'unsettling', 'is', 'the', 'francie', \"'\", 's', 'overall', 'wink', '-', 'wink', 'yet', 'matter', '-', 'of', '-', 'fact', 'attitude', 'about', 'everything', ',', 'expressed', 'in', 'a', 'cheeky', 'voiceover', 'narration', 'delivered', 'by', 'the', 'adult', 'francie', '(', 'rea', 'again', ')', '.', 'think', 'heavenly', 'creatures', 'played', 'largely', 'for', 'laughs', ',', 'and', 'you', \"'\", 'll', 'sort', 'of', 'understand', '.', 'anchoring', 'the', 'whole', 'film', 'is', 'the', 'astonishing', 'debut', 'performance', 'of', 'owens', ';', 'love', 'francie', 'or', 'hate', 'him', ',', 'you', 'cannot', 'take', 'your', 'eyes', 'off', 'of', 'owens', '.', 'the', 'butcher', 'boy', 'truly', 'is', 'a', 'twisted', ',', 'unusual', 'film', 'that', 'is', 'bound', 'to', 'make', 'just', 'about', 'anyone', 'uncomfortable', '.', 'in', 'the', 'lobby', 'after', 'the', 'screening', ',', 'i', 'overheard', 'one', 'man', 'raving', 'about', 'how', 'great', 'yet', 'disturbing', 'it', 'was', ';', 'i', 'also', 'heard', 'one', 'particularly', 'offended', 'woman', 'say', 'with', 'disgust', ',', '\"', 'that', 'movie', 'was', 'so', 'unfunny', '!', '\"', '\"', 'i', 'didn', \"'\", 't', 'know', 'what', 'to', 'expect', '.', 'it', \"'\", 's', 'like', 'something', 'you', 'chase', 'for', 'so', 'long', ',', 'but', 'then', 'you', 'don', \"'\", 't', 'know', 'how', 'to', 'react', 'when', 'you', 'get', 'it', '.', 'i', 'still', 'don', \"'\", 't', 'know', 'how', 'to', 'react', '.', '\"', '--', 'michael', 'jordan', ',', 'on', 'winning', 'his', 'first', 'nba', 'championship', 'in', '1991', '.', '.', '.', 'or', ',', 'my', 'thoughts', 'after', 'meeting', 'him', 'on', 'november', '21', ',', '1997'], 'pos')\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(type(documents[0]))\n",
    "print(documents[1])\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 77717), ('the', 76529), ('.', 65876), ('a', 38106), ('and', 35576), ('of', 34123), ('to', 31937), (\"'\", 30585), ('is', 25195), ('in', 21822), ('s', 18513), ('\"', 17612), ('it', 16107), ('that', 15924), ('-', 15595)]\n",
      "253\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "print(all_words.most_common(15)) #the most frequent words and their appearing times\n",
    "print(all_words[\"stupid\"]) #the number of the word \"stupid\" appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
